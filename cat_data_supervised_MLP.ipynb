{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP time series predictions\n",
    "MLP method more or less using this method as a source https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "\n",
    "tldr: Transform dataset into supervised method, make it stationary, transform to scale, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm as tqdm_general\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting series to supervised and scaling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(-i) for i in range(0, lag)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "# speshul scaling\n",
    "def scale2(train_data):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_data)\n",
    "    # transform train\n",
    "    train = train_data.reshape(train_data.shape[0], train_data.shape[1])\n",
    "    train_scaled = scaler.transform(train_data)\n",
    "    return scaler, train_scaled\n",
    "\n",
    "def fit_existing_scaler(train_data, scaler):\n",
    "    train_scaled = scaler.transform(train_data)\n",
    "    return train_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single model per category approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(window_length):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(window_length, activation='relu', input_shape=(window_length,)))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')    \n",
    "    return model\n",
    "\n",
    "def fit_model(model, train, batch_size, nb_epoch):\n",
    "    X, y = train[:, 0:-1], train[:, -1]    \n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\n",
    "def generate_new_window(train_scaled, predictions_scaled, window_length, index):\n",
    "    # sufficient number of predictions exist\n",
    "    if index - window_length >= 0:\n",
    "        new_train = predictions_scaled[-window_length:]\n",
    "        return np.asarray(new_train).reshape((1, window_length))\n",
    "    \n",
    "    # insufficient predictions, use some from last train sequence\n",
    "    else:\n",
    "        train_end = train_scaled[0][-(window_length - index):]\n",
    "        new_train = np.concatenate((train_end, predictions_scaled))\n",
    "        return new_train.reshape((1, window_length))\n",
    "    \n",
    "def make_window_predictions(model, train_scaled, scaler, raw_values, number_of_predictions, window_length):\n",
    "    scaled_predictions = list()\n",
    "    predictions = list()\n",
    "    \n",
    "    X_train = train_scaled[-1, -window_length:].reshape((1, window_length))    \n",
    "    yhat = model.predict(X_train, 1)[0, 0]\n",
    "    scaled_predictions.append(yhat)\n",
    "\n",
    "    yhat = invert_scale(scaler, X_train[0], yhat)\n",
    "    yhat = yhat + raw_values[-1]\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "    # Predict N steps into the FUTURE!\n",
    "    for i in range(1, number_of_predictions):\n",
    "        X = generate_new_window(X_train, scaled_predictions, window_length, i)\n",
    "        \n",
    "        yhat = model.predict(X, 1)[0, 0]\n",
    "        scaled_predictions.append(yhat)\n",
    "        \n",
    "        yhat = invert_scale(scaler, X[0], yhat)\n",
    "        yhat = yhat + predictions[-1]\n",
    "        predictions.append(yhat)   \n",
    "    return predictions\n",
    "\n",
    "# Create sliding window dataset\n",
    "def create_dataset(train_series, window_length):\n",
    "    windowed_set = timeseries_to_supervised(train_series, window_length)\n",
    "    windowed_set = windowed_set.iloc[:-window_length]\n",
    "    return windowed_set\n",
    "\n",
    "# predicts from single time series using categorical model\n",
    "def predict_from_model(train_series, model, scaler, n_predictions=1, window_length=5, batch_size=4, nb_epoch=5):\n",
    "    supervised_values = timeseries_to_supervised(train_series, window_length)\n",
    "    supervised_values = supervised_values.iloc[:-window_length].values\n",
    "    \n",
    "    train_scaled = fit_existing_scaler(supervised_values, scaler)\n",
    "    return make_window_predictions(model, train_scaled, scaler, train_series, n_predictions, window_length)\n",
    "    \n",
    "# fits or predicts, returns either predictions or nothing respectively\n",
    "def fit_full_category(train_series, model, window_length=5, batch_size=4, nb_epoch=5):\n",
    "    scaler, train_scaled = scale2(train_series.values)\n",
    "    fit_model(model, train_scaled, batch_size, nb_epoch)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisite dataframes, dictionaries, and lists for training and predicting processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction lengths for different scopes\n",
    "horizon_lengths = {\"H\" : 48, \"D\" : 14, \"W\" : 13, \"M\" : 18, \"Q\" : 8, \"Y\" : 6}\n",
    "window_lengths = {\"H\" : 48, \"D\" : 24, \"W\" : 16, \"M\" : 16, \"Q\" : 8, \"Y\" : 6}\n",
    "\n",
    "# Scope + category as key for models, i.e daily_finance or w/e\n",
    "datasets = {}\n",
    "models = {}\n",
    "scalers = {}\n",
    "\n",
    "# All filenames for different scopes\n",
    "filenames = os.listdir(\"./data/cut/10000/train/\")\n",
    "#filenames = [\"Hourly.csv\", \"Daily.csv\"]\n",
    "\n",
    "# Results\n",
    "results_frame = pd.DataFrame()\n",
    "\n",
    "# Test series frame for plotting or w/e\n",
    "test_frame = pd.DataFrame()\n",
    "\n",
    "# M4 info for information about categories\n",
    "m4_info = pd.read_csv('./data/M4-info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create windowed dataframes per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    train_set = pd.read_csv('./data/cut/10000/train/' + file)\n",
    "        \n",
    "    # Go through all the time series in the scope \n",
    "    for i in tnrange(1, train_set.shape[0], desc=file):\n",
    "        current_series = train_set.iloc[i, 2:].dropna()\n",
    "        series_name = train_set.iloc[i, 1]\n",
    "        series_info = m4_info.loc[m4_info['M4id'] == series_name]\n",
    "            \n",
    "        category_name = series_info['category'].values[0].lower()\n",
    "        scope_name = series_info['SP'].values[0].lower()\n",
    "        model_key = scope_name + \"_\" + category_name\n",
    "            \n",
    "        n_window = window_lengths[series_name[0].upper()]\n",
    "        \n",
    "        if model_key in datasets:\n",
    "            current_dataset = create_dataset(current_series, n_window)\n",
    "            np_combined_data = np.concatenate([datasets[model_key], current_dataset], axis=0)\n",
    "            datasets[model_key] = pd.DataFrame(np_combined_data)\n",
    "        else:\n",
    "            datasets[model_key] = create_dataset(current_series, n_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loopy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tqdm(datasets.items(), desc=\"Training\")\n",
    "for model_key, dataset in t:\n",
    "    t.set_description(model_key)\n",
    "    t.refresh() \n",
    "    \n",
    "    n_window = window_lengths[model_key[0].upper()]\n",
    "    \n",
    "    if model_key in models:\n",
    "        current_model = models[model_key]\n",
    "    else:\n",
    "        current_model = create_model(window_length=n_window)\n",
    "        models[model_key] = current_model\n",
    "    \n",
    "    model_category_series = datasets[model_key]\n",
    "    scaler = fit_full_category(model_category_series, current_model, window_length=n_window, batch_size=32, nb_epoch=20)\n",
    "    \n",
    "    scalers[model_key] = scaler\n",
    "    models[model_key] = current_model\n",
    "    #assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write models and scalers to files\n",
    "NB! set the batch identificator to whatever is going on at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_batch_identificator = '150518/'\n",
    "models_folder = './models/' + model_batch_identificator\n",
    "scalers_folder = './scalers/' + model_batch_identificator\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n",
    "\n",
    "for model_callsign, model in models.items():\n",
    "    model.save(models_folder + model_callsign + '.h5')\n",
    "    \n",
    "if not os.path.exists(scalers_folder):\n",
    "    os.makedirs(scalers_folder)\n",
    "\n",
    "for scaler_callsign, scaler in scalers.items():\n",
    "    joblib.dump(scaler, scalers_folder + scaler_callsign + '.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANITY IS FOR THE WEAK! (Read models and scalers from files)\n",
    "NB! set the batch identificator to whatever is going on at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_batch_identificator = '150518/'\n",
    "\n",
    "models_folder = './models/' + model_batch_identificator\n",
    "scalers_folder = './scalers/' + model_batch_identificator\n",
    "\n",
    "model_filenames = os.listdir(models_folder)\n",
    "scaler_filenames = os.listdir(scalers_folder)\n",
    "\n",
    "# create a models dict if it doesn't exist and create other required data objects \n",
    "if 'models' not in globals() and 'scalers' not in globals():\n",
    "    models = {}\n",
    "    scalers = {}\n",
    "    horizon_lengths = {\"H\" : 48, \"D\" : 14, \"W\" : 13, \"M\" : 18, \"Q\" : 8, \"Y\" : 6}\n",
    "    window_lengths = {\"H\" : 48, \"D\" : 24, \"W\" : 16, \"M\" : 16, \"Q\" : 8, \"Y\" : 6}\n",
    "\n",
    "    filenames = os.listdir(\"./data/cut/10000/train/\")\n",
    "    #filenames = [\"Hourly.csv\", \"Daily.csv\"]\n",
    "    m4_info = pd.read_csv('./data/M4-info.csv')\n",
    "\n",
    "    results_frame = pd.DataFrame()\n",
    "    test_frame = pd.DataFrame()\n",
    "\n",
    "    for model_file in model_filenames:\n",
    "        models[model_file.split(\".\")[0]] = load_model(models_folder + model_file)\n",
    "    \n",
    "    for scaler_file in scaler_filenames:\n",
    "        scalers[scaler_file.split(\".\")[0]] = joblib.load(scalers_folder + scaler_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loopy predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79472dee174073ad66161e29c7f460"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 49)\n",
      "(1, 48)\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7f3cb58241e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtest_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_from_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-9dfee4e9fa00>\u001b[0m in \u001b[0;36mpredict_from_model\u001b[1;34m(train_series, model, scaler, n_predictions, window_length, batch_size, nb_epoch)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mtrain_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_existing_scaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupervised_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmake_window_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_series\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# fits or predicts, returns either predictions or nothing respectively\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-9dfee4e9fa00>\u001b[0m in \u001b[0;36mmake_window_predictions\u001b[1;34m(model, train_scaled, scaler, raw_values, number_of_predictions, window_length)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in filenames:\n",
    "    train_set = pd.read_csv('./data/cut/10000/train/' + file)\n",
    "    test_set = pd.read_csv('./data/cut/10000/test/' + file)\n",
    "    \n",
    "    # Go through all the time series in the scope \n",
    "    for i in tnrange(1, train_set.shape[0], desc=file):\n",
    "        series_name = train_set.iloc[i, 1]\n",
    "        series_info = m4_info.loc[m4_info['M4id'] == series_name]\n",
    "        \n",
    "        category_name = series_info['category'].values[0].lower()\n",
    "        scope_name = series_info['SP'].values[0].lower()\n",
    "        model_key = scope_name + \"_\" + category_name \n",
    "        \n",
    "        model = models[model_key]\n",
    "        scaler = scalers[model_key]\n",
    "        \n",
    "        n_predictions = horizon_lengths[series_name[0].upper()]\n",
    "        n_window = window_lengths[series_name[0].upper()]\n",
    "        \n",
    "        series = train_set.iloc[i, 2:].dropna()\n",
    "        test_series = test_set.iloc[i, 1:].tolist()\n",
    "        test_series.insert(0, series_name)\n",
    "        \n",
    "        pred = predict_from_model(series, model, scaler, n_predictions, window_length=n_window)\n",
    "        pred.insert(0, series_name)\n",
    "        \n",
    "        results_frame = results_frame.append(pd.Series(pred), ignore_index=True)\n",
    "        test_frame = test_frame.append(pd.Series(test_series), ignore_index=True)   \n",
    "        #assert False\n",
    "    #assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write separate results into .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_frame = results_frame[results_frame[0].str.contains(\"H\")] \n",
    "hourly_frame = hourly_frame.dropna(axis=1, how='all')\n",
    "hourly_frame.to_csv(\"./results/h_results.csv\", index=False)\n",
    "\n",
    "daily_frame = results_frame[results_frame[0].str.contains(\"D\")]\n",
    "daily_frame = daily_frame.dropna(axis=1, how='all')\n",
    "daily_frame.to_csv(\"./results/d_results.csv\", index=False)\n",
    "\n",
    "weekly_frame = results_frame[results_frame[0].str.contains(\"W\")]\n",
    "weekly_frame = weekly_frame.dropna(axis=1, how='all')\n",
    "weekly_frame.to_csv(\"./results/w_results.csv\", index=False)\n",
    "\n",
    "monthly_frame = results_frame[results_frame[0].str.contains(\"M\")]\n",
    "monthly_frame = monthly_frame.dropna(axis=1, how='all')\n",
    "monthly_frame.to_csv(\"./results/m_results.csv\", index=False)\n",
    "\n",
    "quarterly_frame = results_frame[results_frame[0].str.contains(\"Q\")]\n",
    "quarterly_frame = quarterly_frame.dropna(axis=1, how='all')\n",
    "quarterly_frame.to_csv(\"./results/q_results.csv\", index=False)\n",
    "\n",
    "yearly_frame = results_frame[results_frame[0].str.contains(\"Y\")]\n",
    "yearly_frame = yearly_frame.dropna(axis=1, how='all')\n",
    "yearly_frame.to_csv(\"./results/y_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all the resulting data frame to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_frame.to_csv(\"./results/all_results_seq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out a single prediction, if one so chooses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(test_frame.iloc[0, 1:], results_frame.iloc[0, 1:]))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "plt.plot(test_frame.iloc[0, 1:], label=\"Test data\")\n",
    "plt.plot(results_frame.iloc[0, 1:], label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
